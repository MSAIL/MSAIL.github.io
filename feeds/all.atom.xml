<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MSAIL</title><link href="http://msail.github.io/" rel="alternate"></link><link href="http://msail.github.io/feeds/all.atom.xml" rel="self"></link><id>http://msail.github.io/</id><updated>2016-02-10T19:00:00-05:00</updated><entry><title>Round 4</title><link href="http://msail.github.io/events/hack-night/2016/Feb/10/round-4" rel="alternate"></link><updated>2016-02-10T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-10:events/hack-night/2016/Feb/10/round-4</id><summary type="html">&lt;p&gt;Bored of homework? Tired of theory? Itching to actually implement something? Come to hack nights to solve data science challenges, implement a new algorithm, or work on machine learning homework if you need to! This is a great place to collaborate with other MSAIL members!&lt;/p&gt;
&lt;p&gt;This week,
- Spencer will be reading about generalized linear models
- Ben will be implementing the Curriculum Learning paper from Tuesday's reading group&lt;/p&gt;</summary></entry><entry><title>Curriculum Learning</title><link href="http://msail.github.io/events/reading-group/2016/Feb/09/curriculum-learning" rel="alternate"></link><updated>2016-02-09T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-09:events/reading-group/2016/Feb/09/curriculum-learning</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bengio et al. 2009, &lt;a href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf"&gt;"Curriculum Learning"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Humans and animals learn much better when the examples are not randomly presented, but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”.  Complete with examples of from shape recognition, language modeling, and more!&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Regression Part I</title><link href="http://msail.github.io/events/tutorial/2016/Feb/09/regression-part-i" rel="alternate"></link><updated>2016-02-09T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-09:events/tutorial/2016/Feb/09/regression-part-i</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll begin talking about linear and logistic regression, two important linear models in machine learning that exemplify the type of reasoning used throughout the field.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Round 3</title><link href="http://msail.github.io/events/hack-night/2016/Feb/03/round-3" rel="alternate"></link><updated>2016-02-03T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-03:events/hack-night/2016/Feb/03/round-3</id><summary type="html">&lt;p&gt;Bored of homework? Tired of theory? Itching to actually implement something? Come to hack nights to solve data science challenges, implement a new algorithm, or work on machine learning homework if you need to! This is a great place to collaborate with other MSAIL members!  &lt;/p&gt;
&lt;p&gt;Last week,
- Spencer, Cheng Yu, and others continued work on their Markov Chain text prediction project!
- Ben tried to remember how Dirichlet Processes work&lt;/p&gt;
&lt;p&gt;This week, the text prediction project will continue, and a few of us may be working on Google's new deep learning course!&lt;/p&gt;</summary></entry><entry><title>T-SNE</title><link href="http://msail.github.io/events/reading-group/2016/Feb/02/t-sne" rel="alternate"></link><updated>2016-02-02T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-02:events/reading-group/2016/Feb/02/t-sne</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Van der Maaten &amp;amp; Hinton 2008, &lt;a href="http://siplab.tudelft.nl/sites/default/files/vandermaaten08a.pdf"&gt;"Visualizing Data using t-SNE"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scikit-Learn&lt;/strong&gt;:  &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"&gt;T-SNE Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misc&lt;/strong&gt;:  van der Maaten's &lt;a href="http://lvdmaaten.github.io/tsne/"&gt;T-SNE info page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;:  &lt;a href="http://cs.stanford.edu/people/karpathy/tsnejs/"&gt;tSNEJS demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;If data really is as big as they say, it's useful to be able to visualize it!  Us puny humans have trouble visualizing more than three dimensions, so dimensionality reduction techniques are often applied to compress the meaningful information contained in a dataset to a form that's easily interpreted!  Among this interesting class of algorithms are PCA, ICA, LSH, and many more!  Tonight, we'll be talking about t-SNE, a dimensionality reduction algorithm with a clever underlying probabilistic model.&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Klustering without the "K"</title><link href="http://msail.github.io/events/tutorial/2016/Feb/02/klustering-without-the-k" rel="alternate"></link><updated>2016-02-02T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-02:events/tutorial/2016/Feb/02/klustering-without-the-k</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue our discussion of clustering. Last Tuesday, we extended our intuition about clustering and K-Means to a "softer" variant called Gaussian Mixture Modeling.  This week, we'll wrap up our three-week coverage of basic clustering algorithms and introduce a new model, called &lt;strong&gt;Dirichlet Processes&lt;/strong&gt;, that automatically decides the optimal number of clusters as it learns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Round 2</title><link href="http://msail.github.io/events/hack-night/2016/Jan/27/round-2" rel="alternate"></link><updated>2016-01-27T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-27:events/hack-night/2016/Jan/27/round-2</id><summary type="html"></summary></entry><entry><title>Intro to Statistical Learning Theory</title><link href="http://msail.github.io/events/reading-group/2016/Jan/26/intro-to-statistical-learning-theory" rel="alternate"></link><updated>2016-01-26T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-26:events/reading-group/2016/Jan/26/intro-to-statistical-learning-theory</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kulkarni &amp;amp; Harman 2011, &lt;a href="http://www.princeton.edu/~harman/Papers/SLT-tutorial.pdf"&gt;"Statistical Learning Theory:  A Tutorial"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;:  Anthony 1993, &lt;a href="http://www.sciencedirect.com/science/article/pii/0166218X93901269"&gt;"A Result of Vapnik with Applications"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Slides&lt;/strong&gt;:  &lt;a href="http://www-stat.wharton.upenn.edu/~rakhlin/ml_summer_school.pdf"&gt;"MLSS:  Statistical Learning Theory"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lectures&lt;/strong&gt;:  &lt;a href="http://work.caltech.edu/lectures.html#lectures"&gt;"Caltech:  Learning from Data"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Is learning possible?  Yes...  in some cases.  This week we'll investigate when and why machines are able to generalize, viewing the standard machine learning algorithms we know and love under the lens of statistical learning theory, which allows us to understand the strengths and limitations of the algorithms we use every day.  We'll cover important theoretical concepts like VC dimension and PAC learning.&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Clustering the Iris Dataset via K-Means</title><link href="http://msail.github.io/events/tutorial/2016/Jan/26/clustering-the-iris-dataset-via-k-means" rel="alternate"></link><updated>2016-01-26T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-26:events/tutorial/2016/Jan/26/clustering-the-iris-dataset-via-k-means</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue our discussion of clustering. Last Tuesday, we invented and used K-Means to automatically identify flower species.  Today, we'll take a look at K-Means' next of kin: Gaussian Mixtures. A small but important tweak will enable us to cluster datasets far beyond the power of K-Means!  Time permitting, we'll further vary the themes of doubt, iteration, and similarity to understand and apply several related unsupervised learning algorithms.&lt;/p&gt;
&lt;p&gt;By the end of our session, you'll:
 * Appreciate the utility of &lt;em&gt;doubt&lt;/em&gt; in an algorithm, hence entering the Cult of Bayes.
 * Understand multiple clustering algorithms, and when to use which.
 * Enthuse to participate in more of MSAIL's activities!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Clustering the Iris Dataset via K-Means</title><link href="http://msail.github.io/events/tutorial/2016/Jan/26/clustering-the-iris-dataset-via-k-means" rel="alternate"></link><updated>2016-01-26T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-26:events/tutorial/2016/Jan/26/clustering-the-iris-dataset-via-k-means</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue our discussion of clustering. Last Tuesday, we invented and used K-Means to automatically identify flower species.  Today, we'll take a look at K-Means' next of kin: Gaussian Mixtures. A small but important tweak will enable us to cluster datasets far beyond the power of K-Means!  Time permitting, we'll further vary the themes of doubt, iteration, and similarity to understand and apply several related unsupervised learning algorithms.&lt;/p&gt;
&lt;p&gt;By the end of our session, you'll:
 * Appreciate the utility of &lt;em&gt;doubt&lt;/em&gt; in an algorithm, hence entering the Cult of Bayes.
 * Understand multiple clustering algorithms, and when to use which.
 * Enthuse to participate in more of MSAIL's activities!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Kickoff, Markov Models, and Spectral Clustering</title><link href="http://msail.github.io/events/hack-night/2016/Jan/20/kickoff-markov-models-and-spectral-clustering" rel="alternate"></link><updated>2016-01-20T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-20:events/hack-night/2016/Jan/20/kickoff-markov-models-and-spectral-clustering</id><summary type="html">&lt;p&gt;Join us for our first ever hack night!  Bored of homework?  Tired of theory?  Itching to actually &lt;em&gt;implement something&lt;/em&gt;?  Come to hack nights to solve data science challenges, implement a new algorithm, or work on machine learning homework if you need to!  This is a great place to collaborate with other MSAIL members!&lt;/p&gt;</summary></entry><entry><title>Spectral Clustering</title><link href="http://msail.github.io/events/reading-group/2016/Jan/19/spectral-clustering" rel="alternate"></link><updated>2016-01-19T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-19:events/reading-group/2016/Jan/19/spectral-clustering</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Luxburg 2007, &lt;a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf"&gt;"A Tutorial on Spectral Clustering"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Charles Martin, &lt;a href="https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/"&gt;Spectral Clustering:  A Quick Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scikit-Learn,  &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"&gt;Spectral Clustering Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Spectral clustering is a competitive, easy-to-implement clustering method that draws on a lot of interesting theory.  I think this paper is perfect for our first meeting of the semester for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spectral clustering is based on linear algebra, graph theory, and a classic problem in computer science!  &lt;/li&gt;
&lt;li&gt;It's the perfect example of an academic paper.  Some parts are difficult to read, but overall there is plenty of intuition and good discussion.&lt;/li&gt;
&lt;li&gt;The algorithm is presented from several different points of view, which will hopefully lead to good discussion.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a &lt;strong&gt;difficult&lt;/strong&gt; paper, but don't worry!  We're here to help.  Especially if you're new to this, the hope is that you will get practice reading papers, even if you don't understand everything.  We certainly don't!  At the meeting, we'll work through the paper together and try to build up some intuition.  The best thing to do is ask lots of questions!&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Clustering the Iris Dataset via K-Means</title><link href="http://msail.github.io/events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means" rel="alternate"></link><updated>2016-01-19T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-19:events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This Tuesday, join &lt;strong&gt;Aaron Pollack&lt;/strong&gt; and &lt;strong&gt;Samuel Tenka&lt;/strong&gt; for an overview of machine learning and a brief survey of its various applications. Then we'll dive into
a hands-on Machine Learning project, in which we'll:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;visualize a dataset of flower measurements,&lt;/li&gt;
&lt;li&gt;wonder how to group the data-points into species,&lt;/li&gt;
&lt;li&gt;formalize this wonder as a problem in "Clustering",&lt;/li&gt;
&lt;li&gt;invent a fully-automatic Clustering algorithm,&lt;/li&gt;
&lt;li&gt;implement that algorithm using standard packages, and&lt;/li&gt;
&lt;li&gt;analyze algorithm performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the end of our session, you'll have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Mental map of Machine Learning's lay of the land.&lt;/li&gt;
&lt;li&gt;Experience using standard packages to analyze real data.&lt;/li&gt;
&lt;li&gt;Enthusiasm to participate in more of MSAIL's activities!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.&lt;/p&gt;</summary></entry><entry><title>Clustering the Iris Dataset via K-Means</title><link href="http://msail.github.io/events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means" rel="alternate"></link><updated>2016-01-19T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-19:events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This Tuesday, join &lt;strong&gt;Aaron Pollack&lt;/strong&gt; and &lt;strong&gt;Samuel Tenka&lt;/strong&gt; for an overview of machine learning and a brief survey of its various applications. Then we'll dive into
a hands-on Machine Learning project, in which we'll:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;visualize a dataset of flower measurements,&lt;/li&gt;
&lt;li&gt;wonder how to group the data-points into species,&lt;/li&gt;
&lt;li&gt;formalize this wonder as a problem in "Clustering",&lt;/li&gt;
&lt;li&gt;invent a fully-automatic Clustering algorithm,&lt;/li&gt;
&lt;li&gt;implement that algorithm using standard packages, and&lt;/li&gt;
&lt;li&gt;analyze algorithm performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the end of our session, you'll have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Mental map of Machine Learning's lay of the land.&lt;/li&gt;
&lt;li&gt;Experience using standard packages to analyze real data.&lt;/li&gt;
&lt;li&gt;Enthusiasm to participate in more of MSAIL's activities!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.&lt;/p&gt;</summary></entry></feed>