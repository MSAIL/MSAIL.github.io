<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MSAIL</title><link href="http://msail.github.io/" rel="alternate"></link><link href="http://msail.github.io/feeds/tutorial.atom.xml" rel="self"></link><id>http://msail.github.io/</id><updated>2016-03-08T17:30:00-05:00</updated><entry><title>Support Vector Machines</title><link href="http://msail.github.io/events/tutorial/2016/Mar/08/support-vector-machines" rel="alternate"></link><updated>2016-03-08T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-03-08:events/tutorial/2016/Mar/08/support-vector-machines</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll put to use all the intuition we've built for regression, classification, and high-dimensional data, by developing the Support Vector Machine classification and regression algorithms. Along the way, we'll encounter the perceptron algorithm and the Kernel Trick, which, extended in a different direction, leads to Neural Nets, a subject of future tutorials.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We don't bite!&lt;/p&gt;</summary></entry><entry><title>Regression Part II</title><link href="http://msail.github.io/events/tutorial/2016/Feb/16/regression-part-ii" rel="alternate"></link><updated>2016-02-16T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-16:events/tutorial/2016/Feb/16/regression-part-ii</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue discussing &lt;em&gt;linear regression&lt;/em&gt;, that is, how to use lines to fit a dataset. In particular, how do we deal with large numbers of variables? With nonlinear data? With small datasets, where there is a risk of learning spurious patterns (aka overfitting)? By the end of the session you'll have obtained an elegant weapon against the complexity of data.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt;  Come anyway!  We're continuing with regression, so there's plenty of opportunity to catch up!&lt;/p&gt;</summary></entry><entry><title>Regression Part I</title><link href="http://msail.github.io/events/tutorial/2016/Feb/09/regression-part-i" rel="alternate"></link><updated>2016-02-09T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-09:events/tutorial/2016/Feb/09/regression-part-i</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll begin talking about linear and logistic regression, two important linear models in machine learning that exemplify the type of reasoning used throughout the field.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Klustering without the "K"</title><link href="http://msail.github.io/events/tutorial/2016/Feb/02/klustering-without-the-k" rel="alternate"></link><updated>2016-02-02T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-02:events/tutorial/2016/Feb/02/klustering-without-the-k</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue our discussion of clustering. Last Tuesday, we extended our intuition about clustering and K-Means to a "softer" variant called Gaussian Mixture Modeling.  This week, we'll wrap up our three-week coverage of basic clustering algorithms and introduce a new model, called &lt;strong&gt;Dirichlet Processes&lt;/strong&gt;, that automatically decides the optimal number of clusters as it learns.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Gaussian Mixture Modeling</title><link href="http://msail.github.io/events/tutorial/2016/Jan/26/gaussian-mixture-modeling" rel="alternate"></link><updated>2016-01-26T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-26:events/tutorial/2016/Jan/26/gaussian-mixture-modeling</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!&lt;/strong&gt;  This Tuesday, we'll continue our discussion of clustering. Last Tuesday, we invented and used K-Means to automatically identify flower species.  Today, we'll take a look at K-Means' next of kin: Gaussian Mixtures. A small but important tweak will enable us to cluster datasets far beyond the power of K-Means!  Time permitting, we'll further vary the themes of doubt, iteration, and similarity to understand and apply several related unsupervised learning algorithms.&lt;/p&gt;
&lt;p&gt;By the end of our session, you'll:
 * Appreciate the utility of &lt;em&gt;doubt&lt;/em&gt; in an algorithm, hence entering the Cult of Bayes.
 * Understand multiple clustering algorithms, and when to use which.
 * Enthuse to participate in more of MSAIL's activities!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;What if I missed last week?&lt;/strong&gt; Answer: &lt;em&gt;come anyway&lt;/em&gt;! We're still on the topic of clustering, so now's the perfect time to catch up. In fact, this week's algorithm is secretly the same as last time's, except for a small but important tweak!&lt;/p&gt;</summary></entry><entry><title>Clustering the Iris Dataset via K-Means</title><link href="http://msail.github.io/events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means" rel="alternate"></link><updated>2016-01-19T17:30:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-19:events/tutorial/2016/Jan/19/clustering-the-iris-dataset-via-k-means</id><summary type="html">&lt;p&gt;&lt;strong&gt;Interested in Machine Learning?  Don't know where to start?  Never fear!!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This Tuesday, join &lt;strong&gt;Aaron Pollack&lt;/strong&gt; and &lt;strong&gt;Samuel Tenka&lt;/strong&gt; for an overview of machine learning and a brief survey of its various applications. Then we'll dive into
a hands-on Machine Learning project, in which we'll:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;visualize a dataset of flower measurements,&lt;/li&gt;
&lt;li&gt;wonder how to group the data-points into species,&lt;/li&gt;
&lt;li&gt;formalize this wonder as a problem in "Clustering",&lt;/li&gt;
&lt;li&gt;invent a fully-automatic Clustering algorithm,&lt;/li&gt;
&lt;li&gt;implement that algorithm using standard packages, and&lt;/li&gt;
&lt;li&gt;analyze algorithm performance.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;By the end of our session, you'll have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A Mental map of Machine Learning's lay of the land.&lt;/li&gt;
&lt;li&gt;Experience using standard packages to analyze real data.&lt;/li&gt;
&lt;li&gt;Enthusiasm to participate in more of MSAIL's activities!&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The problem is computationally difficult (NP-hard); however, there are efficient heuristic algorithms that are commonly employed and converge quickly to a local optimum. These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions via an iterative refinement approach employed by both algorithms. Additionally, they both use cluster centers to model the data; however, k-means clustering tends to find clusters of comparable spatial extent, while the expectation-maximization mechanism allows clusters to have different shapes.&lt;/p&gt;</summary></entry></feed>