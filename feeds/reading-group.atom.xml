<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>MSAIL</title><link href="http://msail.github.io/" rel="alternate"></link><link href="http://msail.github.io/feeds/reading-group.atom.xml" rel="self"></link><id>http://msail.github.io/</id><updated>2016-02-16T19:00:00-05:00</updated><entry><title>Error-Correcting Output Codes</title><link href="http://msail.github.io/events/reading-group/2016/Feb/16/error-correcting-output-codes" rel="alternate"></link><updated>2016-02-16T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-16:events/reading-group/2016/Feb/16/error-correcting-output-codes</id><summary type="html">&lt;h3&gt;Core Papers (Pick One!)&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Dietterich &amp;amp; Bakiri, "Error-Correcting Output Codes:  A General Method for Improving Multiclass Inductive Learning Programs"&lt;/li&gt;
&lt;li&gt;Dietterich &amp;amp; Bakiri, "Solving Multiclass Learning Problems via Error-Correcting Output Codes"&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set of classes.  This paper compares existing approaches to a new technique in which error-correcting codes are employed as a distributed representation.  We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks.  WE also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning.  Finally, the error-correcting code technique can provide reliable class probability estimates.&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Keywords:&lt;/strong&gt; Error-correcting codes, decision trees, random forests, information theory&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Presented by Alex Chojnacki.&lt;/em&gt;&lt;/p&gt;</summary></entry><entry><title>Curriculum Learning</title><link href="http://msail.github.io/events/reading-group/2016/Feb/09/curriculum-learning" rel="alternate"></link><updated>2016-02-09T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-09:events/reading-group/2016/Feb/09/curriculum-learning</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Bengio et al. 2009, &lt;a href="http://ronan.collobert.com/pub/matos/2009_curriculum_icml.pdf"&gt;"Curriculum Learning"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Humans and animals learn much better when the examples are not randomly presented, but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them “curriculum learning”.  Complete with examples of from shape recognition, language modeling, and more!&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>T-SNE</title><link href="http://msail.github.io/events/reading-group/2016/Feb/02/t-sne" rel="alternate"></link><updated>2016-02-02T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-02-02:events/reading-group/2016/Feb/02/t-sne</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Van der Maaten &amp;amp; Hinton 2008, &lt;a href="http://siplab.tudelft.nl/sites/default/files/vandermaaten08a.pdf"&gt;"Visualizing Data using t-SNE"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Scikit-Learn&lt;/strong&gt;:  &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html"&gt;T-SNE Documentation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Misc&lt;/strong&gt;:  van der Maaten's &lt;a href="http://lvdmaaten.github.io/tsne/"&gt;T-SNE info page&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Demo&lt;/strong&gt;:  &lt;a href="http://cs.stanford.edu/people/karpathy/tsnejs/"&gt;tSNEJS demo&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;If data really is as big as they say, it's useful to be able to visualize it!  Us puny humans have trouble visualizing more than three dimensions, so dimensionality reduction techniques are often applied to compress the meaningful information contained in a dataset to a form that's easily interpreted!  Among this interesting class of algorithms are PCA, ICA, LSH, and many more!  Tonight, we'll be talking about t-SNE, a dimensionality reduction algorithm with a clever underlying probabilistic model.&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Intro to Statistical Learning Theory</title><link href="http://msail.github.io/events/reading-group/2016/Jan/26/intro-to-statistical-learning-theory" rel="alternate"></link><updated>2016-01-26T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-26:events/reading-group/2016/Jan/26/intro-to-statistical-learning-theory</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Kulkarni &amp;amp; Harman 2011, &lt;a href="http://www.princeton.edu/~harman/Papers/SLT-tutorial.pdf"&gt;"Statistical Learning Theory:  A Tutorial"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Paper&lt;/strong&gt;:  Anthony 1993, &lt;a href="http://www.sciencedirect.com/science/article/pii/0166218X93901269"&gt;"A Result of Vapnik with Applications"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Slides&lt;/strong&gt;:  &lt;a href="http://www-stat.wharton.upenn.edu/~rakhlin/ml_summer_school.pdf"&gt;"MLSS:  Statistical Learning Theory"&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Lectures&lt;/strong&gt;:  &lt;a href="http://work.caltech.edu/lectures.html#lectures"&gt;"Caltech:  Learning from Data"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Is learning possible?  Yes...  in some cases.  This week we'll investigate when and why machines are able to generalize, viewing the standard machine learning algorithms we know and love under the lens of statistical learning theory, which allows us to understand the strengths and limitations of the algorithms we use every day.  We'll cover important theoretical concepts like VC dimension and PAC learning.&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry><entry><title>Spectral Clustering</title><link href="http://msail.github.io/events/reading-group/2016/Jan/19/spectral-clustering" rel="alternate"></link><updated>2016-01-19T19:00:00-05:00</updated><author><name>MSAIL</name></author><id>tag:msail.github.io,2016-01-19:events/reading-group/2016/Jan/19/spectral-clustering</id><summary type="html">&lt;h3&gt;Core Paper&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Luxburg 2007, &lt;a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf"&gt;"A Tutorial on Spectral Clustering"&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Additional Resources&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Charles Martin, &lt;a href="https://charlesmartin14.wordpress.com/2012/10/09/spectral-clustering/"&gt;Spectral Clustering:  A Quick Overview&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Scikit-Learn,  &lt;a href="http://scikit-learn.org/stable/modules/generated/sklearn.cluster.SpectralClustering.html"&gt;Spectral Clustering Documentation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;Summary&lt;/h3&gt;
&lt;p&gt;Spectral clustering is a competitive, easy-to-implement clustering method that draws on a lot of interesting theory.  I think this paper is perfect for our first meeting of the semester for several reasons:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Spectral clustering is based on linear algebra, graph theory, and a classic problem in computer science!  &lt;/li&gt;
&lt;li&gt;It's the perfect example of an academic paper.  Some parts are difficult to read, but overall there is plenty of intuition and good discussion.&lt;/li&gt;
&lt;li&gt;The algorithm is presented from several different points of view, which will hopefully lead to good discussion.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is a &lt;strong&gt;difficult&lt;/strong&gt; paper, but don't worry!  We're here to help.  Especially if you're new to this, the hope is that you will get practice reading papers, even if you don't understand everything.  We certainly don't!  At the meeting, we'll work through the paper together and try to build up some intuition.  The best thing to do is ask lots of questions!&lt;/p&gt;
&lt;p&gt;This semester, we are experimenting with a new format.  There will be a longer, 10-20 minute presentation at the start of each meeting, followed by the usual discussion.  We hope that the longer presentation will help those who struggle with reading academic papers to see how more experienced members approach reading papers.&lt;/p&gt;</summary></entry></feed>