<!DOCTYPE html>

<html>
    <head>
        <!-- Metadata -->
    	<meta charset="utf-8">
    
        <!-- Title -->
        <title>MSAIL</title>
    
        <!-- Google Fonts -->
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic,600,600italic' rel='stylesheet' type='text/css'>
    
        <!-- CSS -->
        <link rel="stylesheet" href="theme/css/style.css" type="text/css">
    </head>
    
    <body>
    	<!-- Page Header -->
        <div id="page-header"><div id="header-center"><div id="header-content">
            <!-- Logo -->
    		<a id="header-logo" href="index.html">
    			<img src="http://msail.github.io/theme/images/msail-logo-head.png">
    		</a>
    		<!-- Header Text -->
            <div id="header-text"><div>
                <a id="header-title" href="index.html">
                    Michigan Student Artificial Intelligence Lab
                </a>
                <ul id="header-nav">
    					<li><a href="activities.html">
    						Activities
    					</a></li>
    					<li><a href="blog.html">
    					    Blog
    					</a></li>
    					<li><a href="resources.html">
    						Resources
    					</a></li>
                </ul>
            </div></div>
        </div></div></div>
    
    	<!-- Page Body -->
    	<div id="page-wrap">
    		<!-- Page Content -->
    	    <div id="page-content">
                <p>
                    The <b>Michigan Student Artificial Intelligence Lab (MSAIL)</b>
                    is a reading group on Machine Learning. As
                    <a href="http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html">Andrew Ng said</a>:
    
                    <blockquote>
                        <span class="quote">&ldquo;</span>
                        ...if you read research papers consistently, if you seriously study
                        half a dozen papers a week and you do that for two years, after
                        those two years you will have learned a lot... But that sort of
                        investment, if you spend a whole Saturday studying rather than
                        watching TV, there's no one there to pat you on the back or tell
                        you you did a good job.
                        <span class="quote">&rdquo;</span> &emsp;&mdash;&nbsp; Andrew Ng
                    </blockquote>
                    
                    MSAIL is a community in which motivated students can read
                    and discuss modern machine learning literature together. 
                    We welcome students of all backgrounds and ability.  To join
                    MSAIL and stay up to date, simply join our  
                    <a href="https://join.slack.com/t/msail-team/shared_invite/enQtMjUwMzc4NzUxMTQwLWVjMGMyMDMyYjFmOTgyZjU4MjlhZmQzODE0MzEyNjBmYWRiM2E3ZGQwZWZhNDM1N2E2YWVkMjIxMGI3ZjBiZTk">Slack team</a>!

                    Also, be sure to check out our sister organization: the
                    <a href="https://sites.google.com/umich.edu/mdst-project/">Michigan Data Science Team</a>!
                </p>
    
                <!-- Upcoming Events -->
                <h1>Upcoming Events</h1>
                
                <div class="event-list">
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 17
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1HXvVK3PQskDtVT0mGuxO-f8poaZ8vZrAlN6qLX7hAwc/edit">Variational Inference</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-17) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    How do we do Bayesian inference on models such as the
                                    Hierarchical Dirichlet Processes seen in past weeks? Markov
                                    Chain Monte Carlo is asymptotically exact but can require a
                                    prohibitively large number steps.  Variational Inference
                                    promises to solve a subset of these Bayesian inference problems
                                    more efficiently albeit approximately.  However, the common
                                    formulation of VI known as Mean Field Variational Bayes fails
                                    to accurately estimate its level of uncertainty and
                                    correlations between variables.  The Giordano and Broderick
                                    paper proposes a fix inspired from Statistical Physics.
                                </p>

                            </div>
                        </div>
                
                    </div>           


                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-tut">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    SUN
                                </div>
                                <div class="fancy-monthday">
                                    Oct 15
                                </div>
                            </div>
                            <div class="event-icon-tut"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Tutorial</span>
                                    <span class="event-name"> <a href="https://gitlab.eecs.umich.edu/MSAIL/tensorflow-tutorial">Tensorflow Tutorial: Part B</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Sunday (2017-10-15) at 15:30 in BBB 4901
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Part B will continue the basics developed in Part A by
                                    experimenting with <b>depth</b>, a key characteristic of Deep
                                    Learning.  Depth just means using function composition to our
                                    advantage.  Two great examples of how this affects architecture
                                    lie in Recurrent Networks and Feature Learning.  Both of these
                                    ideas pop up in language models, so our data next week will be
                                    extracts of <em>The Simpson's</em> scripts.
                                </p>

                            </div>
                        </div>
                
                    </div>                   
                </div>


                
                <!-- Recent Events -->
                <h1>Recent Events</h1>


                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-proj">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    WED 
                                </div>
                                <div class="fancy-monthday">
                                    Oct 11
                                </div>
                            </div>
                            <div class="event-icon-proj"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Project</span>
                                    <span class="event-name"> MIDAS Symposium <a href="http://midas.umich.edu/wp-content/uploads/sites/3/2016/09/poster-booklet-2017.pdf">Poster Session</a> </span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Wednesday (2017-10-11) at 12:00 in Michigan League, Kalamazoo Room
                                </div>
                            </div>
                
                            <div class="event-body">
                                Our Compression Project team will display their work (stand #44) at the Poster Session!
                            </div>
                        </div>            
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 10 
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/a/umich.edu/document/d/1WH7Qw57FSuYBTLvnD0-3cD-_EI4m_iHqhPDkP77fSQQ">Deep Q-Learning</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-10) at 18:00 in BBB 3901
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    The promise and challenge of reinforcement learning (RL) lies
                                    in the goal of learning complex behaviors from sparse feedback.
                                    This paucity of feedback has led Yann LeCun to label RL the
                                    "<a href="https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0">cherry on top of the cake</a>"
                                    of Machine Learning: an inconsequential decoration that does
                                    not address deeper issues of generalization.  Yet, by combining
                                    neural networks originally conceived for function approximation
                                    (i.e. supervised learning) with RL updates, deep RL has
                                    achieved fabulous success.  
                                </p>
                                <p>
                                    Deep Q-networks (DQNs) learn policies from high-dimensional
                                    experience via end-to-end differentiable RL.  DeepMind has
                                    shown that DQN agents, receiving only a stream of pixels and
                                    game scores as inputs, surpass the performance of all previous
                                    algorithms and often match human performance in a suite of 49
                                    <a href="https://en.wikipedia.org/wiki/List_of_Atari_2600_games">Atari 2600 games</a>.
                                    DQN is thus the first artificial agent capable of learning to
                                    excel at a tasks as diverse and challenging as Alien War and
                                    Igloo-Building. 
                                </p>
                                <p>
                                    Join us this Tuesday to discuss the techniques and significance
                                    of Deep RL.  Is RL just a cherry on top of the cake, or is it a
                                    scaffold without which the cake has no intelligent form?
                                </p>

                            </div>
                        </div>            
                    </div>



                
            		<div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-tut">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    SUN
                                </div>
                                <div class="fancy-monthday">
                                    Oct 08
                                </div>
                            </div>
                            <div class="event-icon-tut"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Tutorial</span>
                                    <span class="event-name"><a href="https://gitlab.eecs.umich.edu/MSAIL/tensorflow-tutorial">TensorFlow Tutorial: Part A</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Sunday (2017-10-08) at 15:30 in BBB 3941
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
				    This tutorial series aims to teach beginners the fundamentals of TensorFlow. 
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 03 
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/146vit8EQRtgLZeweH6COl8OhPIw1RdlN9vFgm2Jp_go">Neural Machine Translation</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-03) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Neural networks are dense, parametric, and continuous, while
                                    language is sparse, non-parametric, and discrete.  So how can
                                    the former process the latter?
                                </p>
                                <p>
                                    Famously, one uses one-hot embeddings and softmax sampling to
                                    translate between continuous and discrete domains.  One uses
                                    word embeddings to represent sparse sets of words as dense
                                    clouds of semantic vectors.  One use recurrent neural networks
                                    to reduce variable-length sequence problems to local,
                                    parametric ones.
                                </p>
                                <p>
                                    But there has been another breakthrough recently: one can use
                                    Attention Mechanisms to model long-distance relationships
                                    between words!  Attention lies at the core of this week's
                                    papers.
                                </p>
                            </div>
                        </div>
                    </div>

                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 26
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1o9TFOExAkJyHlpnUZuU7m_l4xonUON9_VHaFrwd9NpY">Hierarchical Dirichlet Processes</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-26) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    k-means is the root of all clustering algorithms.  It
                                    generalizes in the following ways: (a) soft cluster
                                    assignments, (b) variable (i.e. inferred) numbers of clusters
                                    and, (c) cluster sets instead of points (e.g. cluster documents
                                    considered as bags of words).  Hierarchical Dirichlet Models
                                    incorporate all 3 types of fanciness and provide a principled,
                                    interpretable way to cluster and analyze documents within a
                                    text corpus.
                                </p>
                            </div>
                        </div>
                    </div>
                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 19
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1yX1hM0dHIAWcGE2NA-9NhszINW20d3uLlXzHCdpciCs">Modern Deep Learning</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-19) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    The past seven decades have seen wave after wave of Artificial
                                    Intelligence <i>nostra</i> (that is, pet schemes or favorite
                                    remedies).  Just think of Perceptrons, Symbolic Methods, Expert
                                    Systems, Probabilistic Graphical Models, and Deep Learning.
                                    While none have proven able to model general intelligence, each
                                    brings its own advantages and limitations.  The latest wave is
                                    Deep Learning.  Join us and the ever-excellent Chengyu Dai as
                                    he speaks on his summer experience at Princeton studying modern
                                    Deep Learning from a theoretical point of view.
                                </p>
                            </div>
                        </div>
                    </div>
                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 12
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1jW2eGRtbiK4KVp8WlKj8Dr-q8ReeE1VwnQKy32zjgps">Statistical Learning Theory</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-12) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Why does science work?  Specifically, if a model does well on
                                    the training set, will it do well on a test set of
                                    never-before-seen examples?  In other words, which models
                                    generalize?  Amazingly, we can derive non-trivial answers to
                                    this question.  For example, it would be nice to say that
                                    "simple models generalize" --- this is Occam's razor, a pillar
                                    of both natural science and "data science".  It turns out we
                                    can give "simple" and "generalize" precise yet motivated
                                    meanings such that Occamâ€™s razor becomes provable.  This week's
                                    papers define and relate a complexity measure (VC dimension) to
                                    a notion of generalization (PAC learnability).
                                </p>
                            </div>
                        </div>
                
                    </div>
                </div>
    
                <!-- Leadership -->
                <h1>Active Leadership</h1>
                
                The following awesome people plan MSAIL's activities.  If you
                would like to help out as well, contact Sam Tenka via email.  
                Our <a href="constitution.html">constitution</a> codifies our
                roles. 
                <div id="leadership">
                    <!-- Laura Balzano -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/laura.jpg">
                        </div>
                        <div class="leader-name"> Laura Balzano </div>
                        <div class="leader-info"> Assistant Professor in EECS </div>
                        <div class="leader-info"> Faculty Mentor </div>
                    </div>

                    <!-- Cheng-Yu Dai -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/cheng-yu.jpg">
                        </div>
                        <div class="leader-name"> Cheng-Yu Dai </div>
                        <div class="leader-info"> PhD, Physics </div>
                        <div class="leader-info"> Literature Selection </div>
                    </div>
                
                    <!-- Vinay Hiremath -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/vinay.jpg">
                        </div>
                        <div class="leader-name"> Vinay Hiremath </div>
                        <div class="leader-info"> BS '18, Computer Science </div>
                        <div class="leader-info"> Social Events </div>
                    </div>

                    <!-- Nori Kojima -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/nori.jpg">
                        </div>
                        <div class="leader-name"> Nori Kojima </div>
                        <div class="leader-info"> BS '19, Computer Science </div>
                        <div class="leader-info"> Project Mentorship </div>
                    </div>

                   <!-- Tejas Prahlad -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/tejas.jpg">
                        </div>
                        <div class="leader-name"> Tejas Prahlad </div>
                        <div class="leader-info"> BS '19, Computer Science </div>
                        <div class="leader-info"> Web Development </div>
                    </div>
                
                    <!-- Jarrid Rector-Brooks -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/jarrid.jpg">
                        </div>
                        <div class="leader-name"> Jarrid Rector-Brooks </div>
                        <div class="leader-info"> BS '18, Computer Science </div>
                        <div class="leader-info"> Speaker Outreach </div>
                    </div>

                    <!-- Sean Stapleton -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/sean.jpg">
                        </div>
                        <div class="leader-name"> Sean Stapleton </div>
                        <div class="leader-info"> BS '21, Computer Science </div>
                        <div class="leader-info"> Majority Whip </div>
                    </div>
                
                    <!-- Pascal Sturmfels -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/pascal.jpg">
                        </div>
                        <div class="leader-name"> Pascal Sturmfels </div>
                        <div class="leader-info"> BS '17, Computer Science </div>
                        <div class="leader-info"> Food and Logistics </div>
                    </div>
                
                    <!-- Sam Tenka -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/sam.jpg">
                        </div>
                        <div class="leader-name"> Sam Tenka </div>
                        <div class="leader-info"> BS '18, Mathematics </div>
                        <div class="leader-info"> Communications </div>
                    </div>

                    <!-- Jenna Wiens -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/jenna.png">
                        </div>
                        <div class="leader-name"> Jenna Wiens </div>
                        <div class="leader-info"> Assistant Professor in CSE </div>
                        <div class="leader-info"> Faculty Mentor </div>
                    </div>
                </div>
        	</div>
    
    	    <!-- Page Footer -->
    	    <div id="page-footer">
    			<span style="float: left">
    				Last updated on 2017-10-04
    			</span>
    	        <span style="float: right">
    				Website by <b>Benjamin &amp; Tejas</b> (prtejas(at)umich(dot)edu)
    			</span>
    	    </div>
    	</div>
    </body>
</html>
