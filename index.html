<!DOCTYPE html>

<html>
    <head>
        <!-- Metadata -->
    	<meta charset="utf-8">
    
        <!-- Title -->
        <title>MSAIL</title>
    
        <!-- Google Fonts -->
        <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic,600,600italic' rel='stylesheet' type='text/css'>
    
        <!-- CSS -->
        <link rel="stylesheet" href="theme/css/style.css" type="text/css">
    </head>
    
    <body>
    	<!-- Page Header -->
        <div id="page-header"><div id="header-center"><div id="header-content">
            <!-- Logo -->
    		<a id="header-logo" href="index.html">
    			<img src="http://msail.github.io/theme/images/msail-logo-head.png">
    		</a>
    		<!-- Header Text -->
            <div id="header-text"><div>
                <a id="header-title" href="index.html">
                    Michigan Student Artificial Intelligence Lab
                </a>
                <ul id="header-nav">
    					<li><a href="activities.html">
    						Activities
    					</a></li>
                                        <!--
    					<li><a href="blog.html">
    					    Blog
    					</a></li>
                                        -->
    					<li><a href="resources.html">
    						Resources
    					</a></li>
                </ul>
            </div></div>
        </div></div></div>
    
    	<!-- Page Body -->
    	<div id="page-wrap">
    		<!-- Page Content -->
    	    <div id="page-content">
                <p>
                    The <b>Michigan Student Artificial Intelligence Lab (MSAIL)</b>
                    is a student organization for discussion of artificial intelligence
                    and machine learning.
                    <a href="http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html">Andrew Ng said</a>:
    
                    <blockquote>
                        <span class="quote">&ldquo;</span>
                        ...if you read research papers consistently, if you seriously study
                        half a dozen papers a week and you do that for two years, after
                        those two years you will have learned a lot... But that sort of
                        investment, if you spend a whole Saturday studying rather than
                        watching TV, there's no one there to pat you on the back or tell
                        you you did a good job.
                        <span class="quote">&rdquo;</span> &emsp;&mdash;&nbsp; Andrew Ng
                    </blockquote>
                    
                    MSAIL is a community in which motivated students can read
                    and discuss modern machine learning literature together. 
                    We welcome students of all backgrounds and ability.  To join
                    MSAIL and stay up to date, simply join our  
                    <a href="https://join.slack.com/t/msail-team/shared_invite/enQtMjUwMzc4NzUxMTQwLWVjMGMyMDMyYjFmOTgyZjU4MjlhZmQzODE0MzEyNjBmYWRiM2E3ZGQwZWZhNDM1N2E2YWVkMjIxMGI3ZjBiZTk">Slack team</a>!

                    Also be sure to check out our sister organization: the
                    <a href="https://sites.google.com/umich.edu/mdst-project/">Michigan Data Science Team</a>! 
                    We are both graciously sponsored by the <a href="http://midas.umich.edu/">Michigan Institute for Data Science</a>.
                </p>
    
                <!-- Upcoming Events -->
                <h1>Upcoming Events</h1>
                

                <div class="event-list">
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Feb 13
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1MuQyGg4oELA3aWEvZtvxYP0yDSLtdlFMEFIk13TSBV4/edit">Time Series Classification</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-02-13) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Since the Delphic oracle, weâ€™ve wondered: what happens next?
                                    In other words, we hoped to extrapolate sequences: to learn
                                    functions from sequences to sequence elements. A generalization
                                    of this is time-series classification: learning functions from
                                    sequences to sets.  For instance, one might want to classify
                                    each transcripts in a grade dataset as belonging to a future
                                    scientist or not.  Or, one might classify heartbeat patterns
                                    as normal or abnormal.
                                </p>
                            </div>
                        </div>
                    </div>
                </div>
                
                <!-- Recent Events -->
                <h1>Recent Events</h1>
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Feb 6
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/12rrPAAs0EiOZerMGKbdzlLFcE_LlL0tzQ5VfmPnAqi4/edit">Architecture Search</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-02-06) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Several decades have passed since the first Neural Network was proposed.
                                    Since then, architectural ideas have proliferated: think of convolution,
                                    gating mechanisms, skip connections, and more. But, while neural network 
                                    architecture has been traditionally handcrafted, some recent efforts aim to partially
                                    automate this process (more like optimizing network architecture on the
                                    dataset). Some early efforts in this direction show that machine-designed
                                    networks outperform hand-crafted networks in vision benchmarks such as CIFAR-10.
                                    We will discuss some of the recent advances!
                                </p>
                            </div>
                        </div>
                    </div>

                     <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Jan 30
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1ePt7-0Tu5qaNtu2z9Ja2X5_KBCDLMUHKSKPcEJzEPNM/edit">Convergence of Gradient Descent</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-01-30) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Why should SGD even converge when you are training Neural Networks?
                                    The choice of optimization algorithm proves to be important, but we
                                    havenâ€™t made much fundamental breakthroughs beyond SGD... until
                                    recently.  To understand why SGD converges, we need to understand the
                                    framework of online learning and how this line of thought can be
                                    generalized to more complicated methods.  Remember, ML = Probability
                                    (or Model) + Optimization. To understand ML, we have to understand when,
                                    how, and why SGD (and its cousins) works. 
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-guest">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Jan 23
                                </div>
                            </div>
                            <div class="event-icon-guest"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Guest Talk</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1-NFFWmZfWiOVyueOrqsMICe4QEhZ4OesZSRggJMPEvw/edit">Learning to Reason</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-01-23) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Deep networks appear to go from questions to answers. But how do they
                                    do so? This week, we will ponder that question both from a forward
                                    view (which asks, given a question, what chain of reasoning might
                                    lead to an answer) and from a backward view (which asks, given a
                                    question and an answer, what chain of reasoning led between them).
                                    To the extent that deep neural networks think (which they do not yet
                                    do), the models we will discuss thus think about thinking! 
                                    <a href="http://rohrbach.vision/"> Dr. Marcus Rohrbach</a> from Facebook
                                    AI Research will join us for this special meeting and discuss his
                                    research and its connection to reasoning.                          
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Jan 16
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1cXbIyVnInAraZN4FpWbX27La_3KK85m_rWMYJL2das0/edit">Causal Inference</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-01-16) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Most machine learning algorithms seek to detect correlations
                                    between variables.  However, <a href="https://xkcd.com/552/"> correlation does not imply causation </a>
                                    Thus, to avoid concluding that ice cream causes
                                    summer or that <a href="http://www.biostat.jhsph.edu/courses/bio621/misc/Chocolate%20consumption%20cognitive%20function%20and%20nobel%20laurates%20(NEJM).pdf">
                                    chocolate consumption accelerates research</a>, we
                                    must rethink our models.  This leads us to the ideas of
                                    <b>structural causal models</b> and <b>propensity</b> scores.  In fact, even
                                    when we do not seek causal conclusions, we may need to model
                                    them.  Indeed, unless we acknowledge that variables under study
                                    might affect a datapointâ€™s visibility, we might think that the
                                    refrigerator light is always on!  Thus, we must correct for
                                    <b>selection bias</b>.  This week, we will overview why and how we can
                                    incorporate the idea of causation into our algorithms!
                                </p>
                            </div>
                        </div>
                    </div> 

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Jan 9
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/16IfLKziCj84VSlFos6Bp_WFq5R8ecAiN1ces8sRhmWc/edit">Capsule Networks</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2018-01-09) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Capsule networks are a new development proposed by Geoffrey Hinton that 
                                    is aimed at correcting some of the major flaws of modern CNN's used in the
                                    field today. The fundamental drawback of the CNN is that it does not consider
                                    the hierarchical relationship between the lower and higher level features
                                    that it detects. Though pooling is used to correct this flaw and help the CNN
                                    focus on certain areas, Hinton regards this as a "disaster". Instead, Hinton
                                    proposes (for the purpose of object detection and classification) to
                                    incorporate the relationships between objects to preserve the hierarchical pose
                                    of objects. This is done through the use of capsules. But does this actually
                                    work? If so, how do you train such a network? Join us for the answers.
                                </p>
                            </div>
                        </div>
                    </div> 


                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Dec 5
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1obGeGEwSg6zN9XN0QDpndhqe8uO31h65VmwkWFFs21g/edit">Associating Images to Natural Languge</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-12-05) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Describing an image in natural language is hard. Especially for computers. 
                                    This task is arguably AI-complete, because describing images requires not 
                                    only perception of objects but also perception of their interrelationships, 
                                    having an understanding of background knowledge of the world and so on. Given
                                    a certain image there a plethora of questions that one could ask that a 
                                    computer would struggle to provide answers for. This week we will focus on 
                                    systems and approaches that bridge the gap between Vision and NLP to provide
                                    useful and descriptive captions to a variety of images. 
                                </p>
                            </div>
                        </div>
                    </div> 

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Nov 28
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1bOwcAOTSVSgLrVUvh3Cmi2bhl5GKP1xpGjwUCjdbSn4/edit">Symbolic Analogies</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-11-28) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    <i>Can computers produce something really new?</i> Though one
                                    might perceive creativity in the strategies discovered by
                                    reinforcement learning Atari-players or in the behavior of
                                    image-valued generative adversarial networks, critics such as
                                    Douglas Hofstadter argue that the deep learning framework misses
                                    something essential. Modern models do rely on linearity for
                                    generalization (which can be thought of as finding word-to-word
                                    analogies in the language domain) rather than trying to find
                                    semantic relationships. Hence, Hofstadter proposes instead to
                                    isolate and study what he argues is cognitions core: abstract
                                    analogy-making. To this end, this weeks papers explore systems
                                    for far-reaching analogy induction in symbolic domains.
                                </p>
                            </div>
                        </div>
                    </div> 

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Nov 21
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/11HS7fwImMpROc6tWfUNiD8tNpuLSFwLddOYjxjtIe68/edit">Evolution Strategies</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-11-21) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Reinforcement learning is the study of learning intelligent behavior rather than
                                    only pattern recognition.  RL has seen many successes with deep learning, such as
                                    AlphaGo.  However, gradient-based optimization may struggle when the reward signal
                                    is sparse.  Evolution Strategies do not suffer from sparse rewards and, relative to
                                    gradient-based RL, are easier to implement, depend on fewer, hyperparameters, scale
                                    better in a distributed setting, and avoid other difficulties of gradient-based RL.
                                    ES could be an important part of building general artificial intelligence.
                                </p>
                            </div>
                        </div>
                    </div>
 
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-proj">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    MON
                                </div>
                                <div class="fancy-monthday">
                                    Nov 20
                                </div>
                            </div>
                            <div class="event-icon-proj"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Social</span>
                                    <span class="event-name"> Movie Night </span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Monday (2017-11-20) at 21:00 in Courtyards South (Broadway 1780)
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Let's enjoy an AI-themed movie before dispersing for Thanksgiving break!
                                    Shall we watch "2001: A Space Odyssey"?  Or perhaps "WarGames?"
                                    Or "Chappie"?  Or "The Matrix"?  Vote on Slack's #random! 
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-proj">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    THU
                                </div>
                                <div class="fancy-monthday">
                                    Nov 16
                                </div>
                            </div>
                            <div class="event-icon-proj"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Social</span>
                                    <span class="event-name"> <a href="http://girlsencoded.eecs.umich.edu/lovelace.html">Ada Lovelace Opera</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Thursday (2017-11-16) at 19:00 in Stamps Auditorium
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    MSAIL is going to attend EECS' "Ada Lovelace Opera: A Celebration of Women in Computing"!
                                    It's free and includes lightning talks and a performance of Tchaikovsky's <em>Enchantress</em>.
                                    If you're interested, pre-register and join the #ada channel on Slack.
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Nov 14
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1HEPRieToe820FlY5wWtDXaNfCh_ZiKycG6U5_Sk-4Iw/edit">Compressive Sensing and Invertibility of CNNs</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-11-14) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Convolutional Neural Networks (CNNs) have had great success in a
                                    number of challenging fields including speech recognition and computer
                                    vision.  Despite this success, experts still do not know why they
                                    work so well.  This means improving a CNN is a result of trial-and-error,
                                    which does not give researchers a lot to work with. The paper that we will
                                    discuss takes a step towards understanding how exactly CNNs work.
                                    Specifically, it looks into inverting CNNs, i.e. reconstructing the
                                    input to a CNN based on hidden activation. In order to show mathematically
                                    that CNNs are invertible, the paper uses ideas from Compressive Sensing,
                                    a subfield of signal processing that deals with acquiring and reconstructing
                                    signals using much fewer samples than traditional signal processing.

                                </p>
                            </div>
                        </div>
                    </div>



                 <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Nov 7
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/10yWVYdWQ_cEOrzDTeMeTYUw-qr6BphDauIrQzDNuRwk/edit">Subspace Clustering</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-11-07) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Subspace clustering assumes and exploits such union-of-subspaces 
                                    structure to discover rich underlying patterns.  Like ordinary
                                    clustering, subspace clustering can be unsupervised or, aided by
                                    some hints, semi-supervised.  Our main reading this week shows how
                                    subspace clustering can request for and incorporate oracle-provided
                                    hints; in other words, the paper proposes to an Active Learning
                                    method for subspace clustering.  One of the authors of this paper
                                    will be the guest speaker this week: John Lipor.
                                </p>
                            </div>
                        </div>
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 31
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1VkIHMsQ2J9KHvmBXaFRBuwyI3c9gMrDoLUWPcW0iAGQ/edit">Neuromorphic Computing</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-31) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                   Neuromorphic (brain-inspired) computing is an emerging technology
                                   that attempts to create chips that more closely mimic the brain.
                                   Memristors are a fairly recent type of hardware component (since 2008)
                                   that functions like a transistor, but also remembers the amount of
                                   charge that previously flowed through it. They also do this
                                   without power. Will using chips that more closely mimic the brain
                                   be the key to brain simulation and locally learnable models without
                                   the need for large amounts of pre-trained data? Come and find out!
                                </p>
                            </div>
                        </div>
                    </div>
 
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 24
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1JF3-5x2kYKHv0U6BlVQLM3XSNrz1NQY3a_9817-885U/edit">Machine Learning in Healthcare</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-24) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Healthcare, healthcare, everywhere!  MSAIL tends to
                                    focus on Machine Learning as an intellectual discipline
                                    whose main current applications are fun demonstrations.
                                    But ML is used in real life, too.  One of its most inspiring
                                    and important applications lies in analyzing the
                                    deluge of data streaming out of medical journals, hospital
                                    records, and real-time patient measurements. However, given
                                    the huge variety of techniques applied to the field, how does
                                    one choose the optimal model while keeping credibility and
                                    actionability in mind?
                                </p>

                            </div>
                        </div>
                    </div> 

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 17
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/1HXvVK3PQskDtVT0mGuxO-f8poaZ8vZrAlN6qLX7hAwc/edit">Variational Inference</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-17) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    How do we do Bayesian inference on models such as the
                                    Hierarchical Dirichlet Processes seen in past weeks? Markov
                                    Chain Monte Carlo is asymptotically exact but can require a
                                    prohibitively large number steps.  Variational Inference
                                    promises to solve a subset of these Bayesian inference problems
                                    more efficiently albeit approximately.  However, the common
                                    formulation of VI known as Mean Field Variational Bayes fails
                                    to accurately estimate its level of uncertainty and
                                    correlations between variables.  The Giordano and Broderick
                                    paper proposes a fix inspired from Statistical Physics.
                                </p>

                            </div>
                        </div>
                
                    </div>           

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-tut">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    SUN
                                </div>
                                <div class="fancy-monthday">
                                    Oct 15
                                </div>
                            </div>
                            <div class="event-icon-tut"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Tutorial</span>
                                    <span class="event-name"> <a href="https://gitlab.eecs.umich.edu/MSAIL/tensorflow-tutorial">Tensorflow Tutorial: Part B</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Sunday (2017-10-15) at 15:30 in BBB 4901
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Part B will continue the basics developed in Part A by
                                    experimenting with <b>depth</b>, a key characteristic of Deep
                                    Learning.  Depth just means using function composition to our
                                    advantage.  Two great examples of how this affects architecture
                                    lie in Recurrent Networks and Feature Learning.  Both of these
                                    ideas pop up in language models, so our data next week will be
                                    extracts of <em>The Simpson's</em> scripts.
                                </p>

                            </div>
                        </div>
                
                    </div>                   

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-proj">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    WED 
                                </div>
                                <div class="fancy-monthday">
                                    Oct 11
                                </div>
                            </div>
                            <div class="event-icon-proj"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Project</span>
                                    <span class="event-name"> MIDAS Symposium <a href="http://midas.umich.edu/wp-content/uploads/sites/3/2016/09/poster-booklet-2017.pdf">Poster Session</a> </span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Wednesday (2017-10-11) at 12:00 in Michigan League, Kalamazoo Room
                                </div>
                            </div>
                
                            <div class="event-body">
                                Our Compression Project team will display their work (stand #44) at the Poster Session!
                            </div>
                        </div>            
                    </div>

                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 10 
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/a/umich.edu/document/d/1WH7Qw57FSuYBTLvnD0-3cD-_EI4m_iHqhPDkP77fSQQ">Deep Q-Learning</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-10) at 18:00 in BBB 3901
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    The promise and challenge of reinforcement learning (RL) lies
                                    in the goal of learning complex behaviors from sparse feedback.
                                    This paucity of feedback has led Yann LeCun to label RL the
                                    "<a href="https://medium.com/intuitionmachine/predictive-learning-is-the-key-to-deep-learning-acceleration-93e063195fd0">cherry on top of the cake</a>"
                                    of Machine Learning: an inconsequential decoration that does
                                    not address deeper issues of generalization.  Yet, by combining
                                    neural networks originally conceived for function approximation
                                    (i.e. supervised learning) with RL updates, deep RL has
                                    achieved fabulous success.  
                                </p>
                                <p>
                                    Deep Q-networks (DQNs) learn policies from high-dimensional
                                    experience via end-to-end differentiable RL.  DeepMind has
                                    shown that DQN agents, receiving only a stream of pixels and
                                    game scores as inputs, surpass the performance of all previous
                                    algorithms and often match human performance in a suite of 49
                                    <a href="https://en.wikipedia.org/wiki/List_of_Atari_2600_games">Atari 2600 games</a>.
                                    DQN is thus the first artificial agent capable of learning to
                                    excel at a tasks as diverse and challenging as Alien War and
                                    Igloo-Building. 
                                </p>
                                <p>
                                    Join us this Tuesday to discuss the techniques and significance
                                    of Deep RL.  Is RL just a cherry on top of the cake, or is it a
                                    scaffold without which the cake has no intelligent form?
                                </p>

                            </div>
                        </div>            
                    </div>



                
            		<div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-tut">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    SUN
                                </div>
                                <div class="fancy-monthday">
                                    Oct 08
                                </div>
                            </div>
                            <div class="event-icon-tut"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Tutorial</span>
                                    <span class="event-name"><a href="https://gitlab.eecs.umich.edu/MSAIL/tensorflow-tutorial">TensorFlow Tutorial: Part A</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Sunday (2017-10-08) at 15:30 in BBB 3941
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
				    This tutorial series aims to teach beginners the fundamentals of TensorFlow. 
                                </p>
                            </div>
                        </div>
                    </div>


                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Oct 03 
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"> <a href="https://docs.google.com/document/d/146vit8EQRtgLZeweH6COl8OhPIw1RdlN9vFgm2Jp_go">Neural Machine Translation</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-10-03) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Neural networks are dense, parametric, and continuous, while
                                    language is sparse, non-parametric, and discrete.  So how can
                                    the former process the latter?
                                </p>
                                <p>
                                    Famously, one uses one-hot embeddings and softmax sampling to
                                    translate between continuous and discrete domains.  One uses
                                    word embeddings to represent sparse sets of words as dense
                                    clouds of semantic vectors.  One use recurrent neural networks
                                    to reduce variable-length sequence problems to local,
                                    parametric ones.
                                </p>
                                <p>
                                    But there has been another breakthrough recently: one can use
                                    Attention Mechanisms to model long-distance relationships
                                    between words!  Attention lies at the core of this week's
                                    papers.
                                </p>
                            </div>
                        </div>
                    </div>

                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 26
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <!-- Event Main -->
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1o9TFOExAkJyHlpnUZuU7m_l4xonUON9_VHaFrwd9NpY">Hierarchical Dirichlet Processes</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-26) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    k-means is the root of all clustering algorithms.  It
                                    generalizes in the following ways: (a) soft cluster
                                    assignments, (b) variable (i.e. inferred) numbers of clusters
                                    and, (c) cluster sets instead of points (e.g. cluster documents
                                    considered as bags of words).  Hierarchical Dirichlet Models
                                    incorporate all 3 types of fanciness and provide a principled,
                                    interpretable way to cluster and analyze documents within a
                                    text corpus.
                                </p>
                            </div>
                        </div>
                    </div>
                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 19
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1yX1hM0dHIAWcGE2NA-9NhszINW20d3uLlXzHCdpciCs">Modern Deep Learning</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-19) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    The past seven decades have seen wave after wave of Artificial
                                    Intelligence <i>nostra</i> (that is, pet schemes or favorite
                                    remedies).  Just think of Perceptrons, Symbolic Methods, Expert
                                    Systems, Probabilistic Graphical Models, and Deep Learning.
                                    While none have proven able to model general intelligence, each
                                    brings its own advantages and limitations.  The latest wave is
                                    Deep Learning.  Join us and the ever-excellent Chengyu Dai as
                                    he speaks on his summer experience at Princeton studying modern
                                    Deep Learning from a theoretical point of view.
                                </p>
                            </div>
                        </div>
                    </div>
                
                    <div class="event-panel">
                        <!-- Event Side -->
                        <div class="event-side-rg">
                            <div class="fancy-date">
                                <div class="fancy-dayofweek">
                                    TUE
                                </div>
                                <div class="fancy-monthday">
                                    Sep 12
                                </div>
                            </div>
                            <div class="event-icon-rg"></div>
                        </div>
                
                        <div class="event-main">
                            <div class="event-header">
                                <!-- Title -->
                                <a class="event-title">
                                    <span class="event-type">Reading Group</span>
                                    <span class="event-name"><a href="https://docs.google.com/document/d/1jW2eGRtbiK4KVp8WlKj8Dr-q8ReeE1VwnQKy32zjgps">Statistical Learning Theory</a></span>
                                </a>
                
                                <!-- Info -->
                                <div class="event-info">
                                    Tuesday (2017-09-12) at 18:00 in BBB 3725
                                </div>
                            </div>
                
                            <div class="event-body">
                                <p>
                                    Why does science work?  Specifically, if a model does well on
                                    the training set, will it do well on a test set of
                                    never-before-seen examples?  In other words, which models
                                    generalize?  Amazingly, we can derive non-trivial answers to
                                    this question.  For example, it would be nice to say that
                                    "simple models generalize" --- this is Occam's razor, a pillar
                                    of both natural science and "data science".  It turns out we
                                    can give "simple" and "generalize" precise yet motivated
                                    meanings such that Occamâ€™s razor becomes provable.  This week's
                                    papers define and relate a complexity measure (VC dimension) to
                                    a notion of generalization (PAC learnability).
                                </p>
                            </div>
                        </div>
                
                    </div>
                </div>
    
                <!-- Leadership -->
                <h1>Active Leadership</h1>
                
                The following awesome people plan MSAIL's activities.  If you
                would like to help out as well, contact Sam Tenka via email.  
                Our <a href="constitution.html">constitution</a> codifies our
                roles. 
                <div id="leadership">
                    <!-- Laura Balzano -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/laura.jpg">
                        </div>
                        <div class="leader-name">Laura Balzano </div>
                        <div class="leader-info"> Assistant Professor in EECS </div>
                        <div class="leader-info"> Faculty Mentor </div>
                    </div>

                    <!-- Cheng-Yu Dai -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/cheng-yu.jpg">
                        </div>
                        <div class="leader-name"> Cheng-Yu Dai </div>
                        <div class="leader-info"> PhD, Physics </div>
                        <div class="leader-info"> Recruitment </div>
                    </div>
                
                    <!-- Vinay Hiremath -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/vinay.jpg">
                        </div>
                        <div class="leader-name"> Vinay Hiremath </div>
                        <div class="leader-info"> BS '18, Computer Science </div>
                        <div class="leader-info"> Archivist </div>
                    </div>

                    <!-- Nori Kojima -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/nori.jpg">
                        </div>
                        <div class="leader-name"> Nori Kojima </div>
                        <div class="leader-info"> BS '19, Computer Science </div>
                        <div class="leader-info"> Tutorials </div>
                    </div>

                    <!-- Danai Koutra -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/danai.jpg">
                        </div>
                        <div class="leader-name"> Danai Koutra </div>
                        <div class="leader-info"> Assistant Professor in CSE </div>
                        <div class="leader-info"> Faculty Mentor </div>
                    </div>

                   <!-- Tejas Prahlad -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/tejas.jpg">
                        </div>
                        <div class="leader-name"> Tejas Prahlad </div>
                        <div class="leader-info"> BS '19, Computer Science </div>
                        <div class="leader-info"> Web Development </div>
                    </div>
                
                    <!-- Jarrid Rector-Brooks -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/jarrid.jpg">
                        </div>
                        <div class="leader-name"> Jarrid Rector-Brooks </div>
                        <div class="leader-info"> BS '18, Computer Science </div>
                        <div class="leader-info"> Communications </div>
                    </div>

                    <!-- Sean Stapleton -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/sean.jpg">
                        </div>
                        <div class="leader-name"> Sean Stapleton </div>
                        <div class="leader-info"> BS '21, Computer Science </div>
                        <div class="leader-info"> Majority Whip </div>
                    </div>
                
                    <!-- Pascal Sturmfels -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/pascal.jpg">
                        </div>
                        <div class="leader-name"> Pascal Sturmfels </div>
                        <div class="leader-info"> BS '17, Computer Science </div>
                        <div class="leader-info"> Food and Logistics </div>
                    </div>
                
                    <!-- Sam Tenka -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/sam.jpg">
                        </div>
                        <div class="leader-name"> Sam Tenka </div>
                        <div class="leader-info"> BS '18, Mathematics </div>
                        <div class="leader-info"> Projects </div>
                    </div>

                    <!-- Jenna Wiens -->
                    <div class="leader-box">
                        <div class="leader-portrait">
                            <img src="theme/images/leadership/jenna.png">
                        </div>
                        <div class="leader-name"> Jenna Wiens </div>
                        <div class="leader-info"> Assistant Professor in CSE </div>
                        <div class="leader-info"> Faculty Mentor </div>
                    </div>
                </div>
        	</div>
    
    	    <!-- Page Footer -->
    	    <div id="page-footer">
    			<span style="float: left">
    				Last updated on 2018-02-11
    			</span>
    	    </div>
    	</div>
    </body>
</html>
