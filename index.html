<!DOCTYPE html>

<html>
<head>
    <!-- Metadata -->
	<meta charset="utf-8">

    <!-- Title -->
    <title>MSAIL</title>

    <!-- Google Fonts -->
    <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700,700italic,400italic,600,600italic' rel='stylesheet' type='text/css'>

    <!-- CSS -->
    <link rel="stylesheet" href="http://msail.github.io/theme/css/style.css" type="text/css">
</head>

<body>
	<!-- Page Header -->
    <div id="page-header"><div id="header-center"><div id="header-content">
        <!-- Logo -->
		<a id="header-logo" href="/">
			<img src="http://msail.github.io/theme/images/msail-logo-head.png">
		</a>
		<!-- Header Text -->
        <div id="header-text"><div>
            <a id="header-title" href="/">
                Michigan Student Artificial Intelligence Lab
            </a>
            <ul id="header-nav">
					<li><a href="http://msail.github.io/schedule">
						Schedule
					</a></li>
					<li><a href="http://msail.github.io/activities">
						Activities
					</a></li>
					<li><a href="http://msail.github.io/resources">
						Resources
					</a></li>
            </ul>
        </div></div>
    </div></div></div>

	<!-- Page Body -->
	<div id="page-wrap">
		<!-- Page Content -->
	    <div id="page-content">

<p>The <b>Michigan Student Artificial Intelligence Lab (MSAIL)</b> focuses on engaging students and researchers from across Michigan with machine learning, data science, and artificial intelligence.  Our purpose is best introduced by the following quote from an <a href="http://www.huffingtonpost.com/2015/05/13/andrew-ng_n_7267682.html">interview with Andrew Ng</a>,

<blockquote>
    <span class="quote">&ldquo;</span> I tell them that if you read research papers consistently, if you seriously study half a dozen papers a week and you do that for two years, after those two years you will have learned a lot... But that sort of investment, if you spend a whole Saturday studying rather than watching TV, there's no one there to pat you on the back or tell you you did a good job. <span class="quote">&rdquo;</span> &emsp;&mdash;&nbsp; Andrew Ng
</blockquote>

We are those people, and we are here to help. Our goal is to keep you motivated and excited about various facets of machine learning, computer vision, natural language processing and data science.  This unique student-led group at Michigan is targeted at students of all backgrounds and ability and, we invite anyone to join us!</p>

For up-to-date event information, please join our Slack team: <a href="http://join.slack.com/t/msail-team/shared_invite/MjM1OTc5MzQ5MDI0LTE1MDQ2NDY1ODYtZjNjNjEzMzIxZQ">msail-team</a>!

<!-- Upcoming Events -->
<h1>Upcoming Events</h1>

<div class="event-list">
    <div class="event-panel">
        <!-- Event Side -->
        <div class="event-side">
            <div class="fancy-date">
                <div class="fancy-dayofweek">
                    TUE
                </div>
                <div class="fancy-monthday">
                    Sep 26
                </div>
            </div>
            <div class="event-icon"></div>
        </div>

        <!-- Event Main -->
        <div class="event-main">
            <div class="event-header">
                <!-- Title -->
                <a class="event-title">
                    <span class="event-type">General Meeting</span>
                    <span class="event-name">Neural Machine Translation</span>
                </a>

                <!-- Info -->
                <div class="event-info">
                    Tuesday (2017-10-03) at 18:00 in BBB 3725
                </div>
            </div>

            <div class="event-body">
                <p>
                    Google uses a neural network to translate phrases that are
                    treated as a series of words, from one language to another.
                    In fact, the entire paragraph is translated from English
                    into Chinese, translated into English into English, it
                    seems very clear!  The great progress in automatic use and
                    the automatic training of the translation system itself is
                    amazing.  Surprisingly, intensive, barometric methods of
                    deep learning can be applied to sporadic, non-amphibious,
                    discrete linguistic data.
                </p>
                <p>
                    Indeed, neural networks are:
                </p>
                <p>,
                    <b>dense</b>: they thrive in data-plentiful regimes and need to see many examples of each feature
                </p>
                <p>
                    <b>parametric</b>: a network’s inputs, weights, and outputs are of fixed size and shape
                </p>
                <p>
                    <b>continuous</b>: we can only backpropagate through floating-point valued tensors
                </p>
                <p>
                    while language is
                </p>
                <p>
                    <b>sparse</b>: many words are rare
                </p>
                <p>
                    <b>non-parametric</b>: sentences vary in length and parse trees vary in shape
                </p>
                <p>
                    <b>discrete</b>: what’s the average of “whale” and “cow”?
                </p>
                <p>
                    Some basic techniques allow us to bridge the gap: use
                    one-hot embeddings and softmax sampling to translate
                    between continuous and discrete domains.  Use word
                    embeddings to represent sparse sets of words as dense
                    clouds of semantic vectors.  Use recurrent neural networks
                    to reduce variable-length sequence problems to local,
                    parametric ones.
                </p>
                <p>
                    But there’s been another breakthrough recently: one can use
                    Attention Mechanisms to model long-distance relationships
                    between words!  This is the core insight behind this week's
                    papers.
                </p>
            </div>
        </div>

    </div>
</div>


<!-- Upcoming Events -->
<h1>Past Events</h1>

<div class="event-list">

    <div class="event-panel">
        <!-- Event Side -->
        <div class="event-side">
            <div class="fancy-date">
                <div class="fancy-dayofweek">
                    TUE
                </div>
                <div class="fancy-monthday">
                    Sep 26
                </div>
            </div>
            <div class="event-icon"></div>
        </div>

        <!-- Event Main -->
        <div class="event-main">
            <div class="event-header">
                <!-- Title -->
                <a class="event-title">
                    <span class="event-type">General Meeting</span>
                    <span class="event-name">Hierarchical Dirichlet Processes:</span>
                </a>

                <!-- Info -->
                <div class="event-info">
                    Tuesday (2017-09-26) at 18:00 in BBB 3725
                </div>
            </div>

            <div class="event-body">
                <p>
                    k-means is the root of all clustering algorithms which we
                    can generalize in the following ways: (a) soft cluster
                    assignments, (b) variable (i.e. inferred) numbers of
                    clusters and, (c) cluster sets instead of points (e.g.
                    cluster documents considered as bags of words).
                    Hierarchical Dirichlet Models incorporate all 3 types of
                    fanciness and provide a principled, effective way to
                    cluster and analyze documents within a text corpus.
                    Ultimately, it all comes back around: we can use these
                    fancy insights to revisit k-means.
                </p>
            </div>
        </div>
    </div>

    <div class="event-panel">
        <!-- Event Side -->
        <div class="event-side">
            <div class="fancy-date">
                <div class="fancy-dayofweek">
                    TUE
                </div>
                <div class="fancy-monthday">
                    Sep 26
                </div>
            </div>
            <div class="event-icon"></div>
        </div>

        <div class="event-main">
            <div class="event-header">
                <!-- Title -->
                <a class="event-title">
                    <span class="event-type">General Meeting</span>
                    <span class="event-name">Modern Deep Learning</span>
                </a>

                <!-- Info -->
                <div class="event-info">
                    Tuesday (2017-09-19) at 18:00 in BBB 3725
                </div>
            </div>

            <div class="event-body">
                <p>
                    The past seven decades have seen wave after wave of
                    Artificial Intelligence nostra (that is, pet schemes or
                    favorite remedies).  Just think of Perceptrons, Symbolic
                    Methods, Expert Systems, Probabilistic Graphical Models,
                    and Deep Learning.  While none have proved able to model
                    general intelligence, each brings its own advantages and
                    limitations.  The latest wave is Deep Learning.  Peculiar
                    to this family of Machine Learning methods is an absence of
                    unifying theory. . . until, perhaps, recently!  Join us and
                    the ever-excellent Chengyu Dai as he speaks on his summer
                    experience at Princeton studying modern Deep Learning from
                    a theoretical point of view.
                </p>
            </div>
        </div>
    </div>

    <div class="event-panel">
        <!-- Event Side -->
        <div class="event-side">
            <div class="fancy-date">
                <div class="fancy-dayofweek">
                    TUE
                </div>
                <div class="fancy-monthday">
                    Sep 26
                </div>
            </div>
            <div class="event-icon"></div>
        </div>

        <div class="event-main">
            <div class="event-header">
                <!-- Title -->
                <a class="event-title">
                    <span class="event-type">General Meeting</span>
                    <span class="event-name">Statistical Learning Theory</span>
                </a>

                <!-- Info -->
                <div class="event-info">
                    Tuesday (2017-09-12) at 18:00 in BBB 3725
                </div>
            </div>

            <div class="event-body">
                <p>
                    In 1930, Paul Dirac quested for elegance and found truth.
                    It was purely by thinking that he predicted the existence
                    of antiparticles.  Experiments detected antiparticles two
                    years later.
                </p>
                <p>
                    Astounded by this fabulous success, we ask: why does
                    science work?  Why, when, and how can we extrapolate
                    observed patterns to novel predictions?  Why does the
                    universe seem so orderly?  The question decomposes into two:
                </p>
                <p>
                        0. Why is the universe patterned at all?
                </p>
                <p>
                        1. When are we able to detect patterns?
                </p>
                <p>
                    We leave (0) to philosophers and investigate (1).  To
                    rephrase (1): if a model does well on the training set,
                    will it do well on a test set of never-before-seen
                    examples?  In other words, which models generalize?
                    Amazingly, we can derive non-trivial answers to this
                    question.  For example, it would be nice to say,
                    "simple models generalize" --- this is Occam's razor, a
                    pillar of both natural science and "data science".  It
                    turns out we can give "simple" and "generalize" precise yet
                    motivated meanings such that Occam’s razor becomes
                    provable.  Specifically, the papers we read will define and
                    relate a complexity measure (VC dimension) to a notion of
                    generalization (PAC learnability).
                </p>
                <p>
                    A thought-provoking example for the neural net inside your
                    skull: inscribe a generic n-gon in a circle and connect
                    each pair of vertices to obtain a complete graph.  How many
                    connected regions does this figure possess? The sequence
                    begins 1, 2, 4, 8, 16, ... --- what's next?
                </p>
            </div>
        </div>

    </div>
</div>

<!-- Leadership -->
<h1>Active Leadership</h1>

The following awesome people put on the various activities hosted by MSAIL.  We
are always looking for more help, so if you're interested in running a
tutorial, presenting a paper, or sharing your ideas, approach any of us at a
meeting or contact Sam Tenka or 
<a href="mailto:msail-planning@umich.edu">msail-planning@umich.edu</a>

<div id="leadership">
    <!-- Cheng-Yu Dai -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/cheng-yu.jpg">
        </div>
        <div class="leader-name">Cheng-Yu Dai</div>
        <div class="leader-info">PhD, Physics</div>
        <div class="leader-info"> </div>
    </div>

    <!-- Vinay Hiremath -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/cheng-yu.jpg">
        </div>
        <div class="leader-name">Vinay Hiremath</div>
        <div class="leader-info"> BS '18, Computer Science </div>
        <div class="leader-info"> </div>
    </div>

   <!-- Tejas Prahlad -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/tejas.jpg">
        </div>
        <div class="leader-name">Tejas Prahlad</div>
        <div class="leader-info">BS '19, Computer Science</div>
        <div class="leader-info"> Web Presence </div>
    </div>

    <!-- Jarrid Rector-Brooks -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/cheng-yu.jpg">
        </div>
        <div class="leader-name">Jarrid Rector-Brooks</div>
        <div class="leader-info"> BS '18, Computer Science </div>
        <div class="leader-info"> Speaker Engagements </div>
    </div>

    <!-- Pascal Sturmfels -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/cheng-yu.jpg">
        </div>
        <div class="leader-name">Pascal Sturmfels</div>
        <div class="leader-info"> BS '17, Computer Science </div>
        <div class="leader-info"> </div>
    </div>

    <!-- Sam Tenka -->
    <div class="leader-box">
        <div class="leader-portrait">
            <img src="http://msail.github.io/theme/images/leadership/sam.jpg">
        </div>
        <div class="leader-name">Sam Tenka</div>
        <div class="leader-info">BS '18, Mathematics</div>
        <div class="leader-info"> Communications </div>
    </div>
</div>

	    </div>

	    <!-- Page Footer -->
	    <div id="page-footer">
			<span style="float: left">
				Last updated on May 20, 2017
			</span>
	        <span style="float: right">
				Website by <b>Benjamin &amp; Tejas</b> (prtejas(at)umich(dot)edu)
			</span>
	    </div>
	</div>
</body>
</html>
